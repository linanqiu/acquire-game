# AI-008: Evaluation Framework

## Metadata
- **Epic**: AI Training
- **Status**: `not-started`
- **Priority**: `medium`
- **Effort**: `M` (1-2 hours)
- **Dependencies**: AI-001, AI-007

## Context

Create a framework to benchmark bots against each other. Track win rates and compute Elo ratings for comparing bot versions.

## Requirements

1. Run tournaments between bots
2. Track win/loss/draw statistics
3. Compute Elo ratings
4. Generate comparison reports

## Acceptance Criteria

- [ ] Run N games between bot pairs
- [ ] Track wins, losses, draws per bot
- [ ] Compute Elo ratings
- [ ] Support multiple bot types
- [ ] Parallel game execution
- [ ] Save results to JSON
- [ ] Generate summary report
- [ ] Unit tests for Elo calculation

## Implementation Notes

### File to Create

**backend/training/evaluator.py**:
```python
import json
import math
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from tqdm import tqdm

from game.game import Game
from game.bot import Bot
from game.mcts_bot import MCTSBot
from game.neural_bot import NeuralBot


@dataclass
class MatchResult:
    bot_a: str
    bot_b: str
    winner: str  # 'a', 'b', or 'draw'
    scores: Dict[str, int]


@dataclass
class BotStats:
    name: str
    wins: int = 0
    losses: int = 0
    draws: int = 0
    elo: float = 1500.0

    @property
    def games_played(self) -> int:
        return self.wins + self.losses + self.draws

    @property
    def win_rate(self) -> float:
        if self.games_played == 0:
            return 0.0
        return self.wins / self.games_played


class Evaluator:
    """Evaluate and compare bot performance."""

    BOT_TYPES = {
        'random': lambda: Bot('easy'),  # Easy bot as random baseline
        'easy': lambda: Bot('easy'),
        'medium': lambda: Bot('medium'),
        'hard': lambda: Bot('hard'),
        'mcts_100': lambda: MCTSBot(100),
        'mcts_500': lambda: MCTSBot(500),
        'mcts_1000': lambda: MCTSBot(1000),
        'neural': lambda: NeuralBot(),
    }

    def __init__(self):
        self.stats: Dict[str, BotStats] = defaultdict(lambda: BotStats(''))
        self.results: List[MatchResult] = []

    def run_tournament(
        self,
        bots: List[str],
        games_per_pair: int = 100,
        parallel: bool = True,
    ) -> Dict[str, BotStats]:
        """Run round-robin tournament between bots."""
        pairs = [(a, b) for i, a in enumerate(bots) for b in bots[i+1:]]

        for bot_a, bot_b in tqdm(pairs, desc="Running matches"):
            for _ in range(games_per_pair):
                result = self._play_match(bot_a, bot_b)
                self.results.append(result)
                self._update_stats(result)

        self._calculate_elo()
        return dict(self.stats)

    def _play_match(self, bot_a_name: str, bot_b_name: str) -> MatchResult:
        """Play one match between two bots."""
        game = Game()

        # Add a third bot to make it 3 players
        game.add_player('a', bot_a_name)
        game.add_player('b', bot_b_name)
        game.add_player('c', 'filler')

        bot_a = self.BOT_TYPES[bot_a_name]()
        bot_b = self.BOT_TYPES[bot_b_name]()
        bot_c = Bot('easy')  # Filler bot

        game.start_game()

        while not game.is_game_over():
            player = game.get_current_player_id()

            if player == 'a':
                action = bot_a.choose_action(game, player)
            elif player == 'b':
                action = bot_b.choose_action(game, player)
            else:
                action = bot_c.choose_action(game, player)

            game.apply_action(action)

        # Determine winner between a and b
        rankings = game.get_final_rankings()
        scores = {r['player_id']: r['total'] for r in rankings}

        a_rank = next(i for i, r in enumerate(rankings) if r['player_id'] == 'a')
        b_rank = next(i for i, r in enumerate(rankings) if r['player_id'] == 'b')

        if a_rank < b_rank:
            winner = 'a'
        elif b_rank < a_rank:
            winner = 'b'
        else:
            winner = 'draw'

        return MatchResult(
            bot_a=bot_a_name,
            bot_b=bot_b_name,
            winner=winner,
            scores=scores,
        )

    def _update_stats(self, result: MatchResult):
        """Update statistics from match result."""
        if self.stats[result.bot_a].name == '':
            self.stats[result.bot_a].name = result.bot_a
        if self.stats[result.bot_b].name == '':
            self.stats[result.bot_b].name = result.bot_b

        if result.winner == 'a':
            self.stats[result.bot_a].wins += 1
            self.stats[result.bot_b].losses += 1
        elif result.winner == 'b':
            self.stats[result.bot_b].wins += 1
            self.stats[result.bot_a].losses += 1
        else:
            self.stats[result.bot_a].draws += 1
            self.stats[result.bot_b].draws += 1

    def _calculate_elo(self):
        """Calculate Elo ratings from results."""
        K = 32  # Elo K-factor

        for result in self.results:
            ra = self.stats[result.bot_a].elo
            rb = self.stats[result.bot_b].elo

            # Expected scores
            ea = 1 / (1 + 10 ** ((rb - ra) / 400))
            eb = 1 / (1 + 10 ** ((ra - rb) / 400))

            # Actual scores
            if result.winner == 'a':
                sa, sb = 1, 0
            elif result.winner == 'b':
                sa, sb = 0, 1
            else:
                sa, sb = 0.5, 0.5

            # Update Elo
            self.stats[result.bot_a].elo += K * (sa - ea)
            self.stats[result.bot_b].elo += K * (sb - eb)

    def save_results(self, path: str):
        """Save results to JSON."""
        data = {
            'stats': {k: asdict(v) for k, v in self.stats.items()},
            'results': [asdict(r) for r in self.results],
        }
        Path(path).write_text(json.dumps(data, indent=2))

    def print_summary(self):
        """Print summary report."""
        print("\n=== Bot Evaluation Summary ===\n")
        print(f"{'Bot':<15} {'Games':>6} {'Wins':>6} {'Win%':>7} {'Elo':>7}")
        print("-" * 45)

        sorted_stats = sorted(
            self.stats.values(),
            key=lambda s: s.elo,
            reverse=True,
        )

        for s in sorted_stats:
            print(f"{s.name:<15} {s.games_played:>6} {s.wins:>6} "
                  f"{s.win_rate:>6.1%} {s.elo:>7.0f}")


if __name__ == '__main__':
    evaluator = Evaluator()
    evaluator.run_tournament(
        bots=['easy', 'medium', 'hard', 'mcts_100'],
        games_per_pair=50,
    )
    evaluator.print_summary()
    evaluator.save_results('evaluation_results.json')
```

## Verification

```bash
cd backend
python -m training.evaluator

# View results
cat evaluation_results.json
```

## Reference

- [AI Roadmap - Evaluation](../../../ai/ROADMAP.md#evaluation-framework)

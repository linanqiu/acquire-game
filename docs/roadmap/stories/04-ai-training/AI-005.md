# AI-005: Policy Network Architecture

## Metadata
- **Epic**: AI Training
- **Status**: `not-started`
- **Priority**: `high`
- **Effort**: `M` (1-2 hours)
- **Dependencies**: AI-003

## Context

Design and implement the neural network architecture for the policy network. Takes encoded state as input and outputs action probabilities.

## Requirements

1. Input: encoded state (~750 dims)
2. Output: action probabilities
3. Handle variable action space with masking
4. Efficient inference (<10ms)

## Acceptance Criteria

- [ ] MLP architecture with configurable layers
- [ ] Action masking for illegal moves
- [ ] Forward pass produces valid probabilities
- [ ] Batch inference supported
- [ ] Model can be saved/loaded
- [ ] Inference <10ms on CPU
- [ ] Unit tests for forward pass

## Implementation Notes

### File to Create

**backend/training/policy_network.py**:
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Optional, Tuple
import numpy as np


class PolicyNetwork(nn.Module):
    """Neural network for Acquire action prediction."""

    def __init__(
        self,
        input_dim: int = 750,
        hidden_dims: List[int] = [256, 256, 128],
        max_actions: int = 200,  # Upper bound on action space
        dropout: float = 0.1,
    ):
        super().__init__()

        self.input_dim = input_dim
        self.max_actions = max_actions

        # Build MLP layers
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim

        self.backbone = nn.Sequential(*layers)

        # Policy head
        self.policy_head = nn.Linear(prev_dim, max_actions)

        # Value head (optional, for actor-critic)
        self.value_head = nn.Linear(prev_dim, 1)

    def forward(
        self,
        state: torch.Tensor,
        action_mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass.

        Args:
            state: (batch, input_dim) encoded game state
            action_mask: (batch, max_actions) binary mask, 1 = legal

        Returns:
            policy: (batch, max_actions) action probabilities
            value: (batch, 1) state value estimate
        """
        features = self.backbone(state)

        # Policy logits
        logits = self.policy_head(features)

        # Apply action mask (set illegal actions to -inf)
        if action_mask is not None:
            logits = logits.masked_fill(action_mask == 0, float('-inf'))

        # Softmax to get probabilities
        policy = F.softmax(logits, dim=-1)

        # Value estimate
        value = self.value_head(features)

        return policy, value

    def get_action(
        self,
        state: np.ndarray,
        action_mask: np.ndarray,
        deterministic: bool = False,
    ) -> int:
        """Get action index for single state."""
        self.eval()

        with torch.no_grad():
            state_t = torch.FloatTensor(state).unsqueeze(0)
            mask_t = torch.FloatTensor(action_mask).unsqueeze(0)

            policy, _ = self.forward(state_t, mask_t)
            probs = policy.squeeze(0).numpy()

            if deterministic:
                return int(np.argmax(probs))
            else:
                return int(np.random.choice(len(probs), p=probs))

    def save(self, path: str):
        """Save model weights."""
        torch.save({
            'model_state': self.state_dict(),
            'config': {
                'input_dim': self.input_dim,
                'max_actions': self.max_actions,
            }
        }, path)

    @classmethod
    def load(cls, path: str) -> 'PolicyNetwork':
        """Load model from file."""
        checkpoint = torch.load(path, map_location='cpu')
        config = checkpoint['config']

        model = cls(
            input_dim=config['input_dim'],
            max_actions=config['max_actions'],
        )
        model.load_state_dict(checkpoint['model_state'])
        return model


class ActionEncoder:
    """Encode actions to indices and back."""

    def __init__(self, max_actions: int = 200):
        self.max_actions = max_actions
        self._action_to_idx = {}
        self._idx_to_action = {}
        self._next_idx = 0

    def encode(self, action) -> int:
        """Get index for action, creating new mapping if needed."""
        key = str(action)
        if key not in self._action_to_idx:
            self._action_to_idx[key] = self._next_idx
            self._idx_to_action[self._next_idx] = action
            self._next_idx += 1
        return self._action_to_idx[key]

    def decode(self, idx: int):
        """Get action from index."""
        return self._idx_to_action.get(idx)

    def get_mask(self, legal_actions: list) -> np.ndarray:
        """Create mask for legal actions."""
        mask = np.zeros(self.max_actions, dtype=np.float32)
        for action in legal_actions:
            idx = self.encode(action)
            if idx < self.max_actions:
                mask[idx] = 1.0
        return mask
```

### Installation
```bash
pip install torch
```

## Verification

```bash
cd backend
python -m pytest tests/test_policy_network.py -v

# Test inference speed
python -c "
import torch
import time
from training.policy_network import PolicyNetwork

model = PolicyNetwork()
state = torch.randn(1, 750)
mask = torch.ones(1, 200)

# Warmup
for _ in range(10):
    model(state, mask)

# Benchmark
start = time.time()
for _ in range(100):
    model(state, mask)
print(f'Avg inference: {(time.time() - start) / 100 * 1000:.2f}ms')
"
```

## Reference

- [AI Roadmap - Neural Bot](../../../ai/ROADMAP.md#phase-2-neural-bot-learning-based)

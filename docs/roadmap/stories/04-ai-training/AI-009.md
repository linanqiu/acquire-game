# AI-009: Decision Transformer (Optional)

## Metadata
- **Epic**: AI Training
- **Status**: `not-started`
- **Priority**: `low`
- **Effort**: `L` (2-4 hours)
- **Dependencies**: AI-004, AI-005

## Context

Implement a Decision Transformer for sequence-based game playing. This is an advanced optional approach that may exceed imitation learning performance.

## Requirements

1. Implement transformer architecture
2. Sequence of (return, state, action) tokens
3. Condition on desired return (winning)
4. Offline training on collected trajectories

## Acceptance Criteria

- [ ] GPT-2 style architecture
- [ ] Context length ~100 timesteps
- [ ] Return conditioning implemented
- [ ] Train on trajectory data
- [ ] Inference with R=1.0 conditioning
- [ ] Beat imitation learning baseline
- [ ] Unit tests for forward pass

## Implementation Notes

### File to Create

**backend/training/decision_transformer.py**:
```python
import torch
import torch.nn as nn
import math
from typing import Optional


class DecisionTransformer(nn.Module):
    """Decision Transformer for game playing."""

    def __init__(
        self,
        state_dim: int = 750,
        action_dim: int = 200,
        hidden_dim: int = 256,
        n_layers: int = 4,
        n_heads: int = 4,
        max_length: int = 100,
        dropout: float = 0.1,
    ):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.max_length = max_length

        # Embeddings
        self.state_embed = nn.Linear(state_dim, hidden_dim)
        self.action_embed = nn.Embedding(action_dim, hidden_dim)
        self.return_embed = nn.Linear(1, hidden_dim)

        # Positional embedding
        self.pos_embed = nn.Parameter(
            torch.zeros(1, max_length * 3, hidden_dim)
        )

        # Transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=n_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            activation='gelu',
            batch_first=True,
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)

        # Output heads
        self.action_head = nn.Linear(hidden_dim, action_dim)

        # Layer norm
        self.ln = nn.LayerNorm(hidden_dim)

    def forward(
        self,
        states: torch.Tensor,      # (batch, seq_len, state_dim)
        actions: torch.Tensor,     # (batch, seq_len)
        returns_to_go: torch.Tensor,  # (batch, seq_len, 1)
        attention_mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Forward pass.

        Returns action logits for the last timestep.
        """
        batch_size, seq_len = states.shape[:2]

        # Embed each modality
        state_embeds = self.state_embed(states)
        action_embeds = self.action_embed(actions)
        return_embeds = self.return_embed(returns_to_go)

        # Interleave: [R1, S1, A1, R2, S2, A2, ...]
        # Shape: (batch, seq_len * 3, hidden_dim)
        tokens = torch.stack([
            return_embeds, state_embeds, action_embeds
        ], dim=2).reshape(batch_size, seq_len * 3, self.hidden_dim)

        # Add positional embedding
        tokens = tokens + self.pos_embed[:, :seq_len * 3, :]

        # Causal mask
        causal_mask = self._get_causal_mask(seq_len * 3, tokens.device)

        # Transformer
        output = self.transformer(tokens, mask=causal_mask)
        output = self.ln(output)

        # Get action predictions (every 3rd token starting at index 1)
        # These correspond to states, predicting next action
        state_positions = torch.arange(1, seq_len * 3, 3)
        action_preds = self.action_head(output[:, state_positions, :])

        return action_preds

    def _get_causal_mask(self, size: int, device) -> torch.Tensor:
        """Create causal attention mask."""
        mask = torch.triu(torch.ones(size, size, device=device), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask

    def get_action(
        self,
        states: torch.Tensor,      # (1, seq_len, state_dim)
        actions: torch.Tensor,     # (1, seq_len)
        returns_to_go: torch.Tensor,  # (1, seq_len, 1)
        action_mask: torch.Tensor,  # (action_dim,)
    ) -> int:
        """Get action for current state."""
        self.eval()

        with torch.no_grad():
            action_preds = self.forward(states, actions, returns_to_go)

            # Get last timestep prediction
            logits = action_preds[0, -1, :]

            # Mask illegal actions
            logits = logits.masked_fill(action_mask == 0, float('-inf'))

            # Sample or argmax
            probs = torch.softmax(logits, dim=-1)
            action = torch.multinomial(probs, 1).item()

        return action


class DTTrainer:
    """Trainer for Decision Transformer."""

    def __init__(
        self,
        model: DecisionTransformer,
        lr: float = 1e-4,
        device: str = 'cpu',
    ):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    def train_step(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        returns_to_go: torch.Tensor,
    ) -> float:
        """Single training step."""
        self.model.train()

        states = states.to(self.device)
        actions = actions.to(self.device)
        returns_to_go = returns_to_go.to(self.device)

        # Forward pass
        action_preds = self.model(states, actions, returns_to_go)

        # Loss: predict actions
        loss = nn.functional.cross_entropy(
            action_preds.reshape(-1, action_preds.size(-1)),
            actions.reshape(-1),
        )

        # Backward
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()

        return loss.item()
```

### Data Format for DT

The training data needs to be reformatted as trajectories with returns-to-go:

```python
def create_dt_dataset(trajectories):
    """Convert game trajectories to DT format."""
    states_all = []
    actions_all = []
    returns_all = []

    for traj in trajectories:
        states = traj['states']  # List of state vectors
        actions = traj['actions']  # List of action indices
        final_return = traj['outcome']  # 1.0 for win, 0.0 for loss

        # Compute returns-to-go (assuming sparse reward at end)
        returns_to_go = [final_return] * len(states)

        states_all.append(states)
        actions_all.append(actions)
        returns_all.append(returns_to_go)

    return states_all, actions_all, returns_all
```

## Verification

```bash
cd backend
python -m pytest tests/test_decision_transformer.py -v

# Quick training test
python -c "
import torch
from training.decision_transformer import DecisionTransformer, DTTrainer

model = DecisionTransformer()
trainer = DTTrainer(model)

# Dummy data
states = torch.randn(2, 10, 750)
actions = torch.randint(0, 200, (2, 10))
returns = torch.ones(2, 10, 1)

loss = trainer.train_step(states, actions, returns)
print(f'Loss: {loss:.4f}')
"
```

## Reference

- [Decision Transformer Paper](https://arxiv.org/abs/2106.01345)
- [AI Roadmap - Decision Transformer](../../../ai/ROADMAP.md#31-option-a-decision-transformer)

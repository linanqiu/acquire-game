# AI-006: Imitation Learning Trainer

## Metadata
- **Epic**: AI Training
- **Status**: `not-started`
- **Priority**: `high`
- **Effort**: `L` (2-4 hours)
- **Dependencies**: AI-004, AI-005

## Context

Implement the training loop for imitation learning (behavioral cloning). Train the policy network to predict actions taken by the teacher bot.

## Requirements

1. Load training data from HDF5
2. Train policy network with cross-entropy loss
3. Validation split for monitoring
4. Learning rate scheduling
5. Early stopping
6. Checkpointing

## Acceptance Criteria

- [ ] Load data from HDF5 file
- [ ] Train/validation split (90/10)
- [ ] Cross-entropy loss for action prediction
- [ ] Adam optimizer with LR scheduling
- [ ] Early stopping on validation loss
- [ ] Save best checkpoint
- [ ] Training completes in <2 hours for 100k samples
- [ ] Validation accuracy >70%
- [ ] Unit tests for training step

## Implementation Notes

### File to Create

**backend/training/imitation_trainer.py**:
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import h5py
from pathlib import Path
from typing import Optional
from tqdm import tqdm

from .policy_network import PolicyNetwork


class ImitationTrainer:
    """Train policy network via imitation learning."""

    def __init__(
        self,
        model: PolicyNetwork,
        lr: float = 1e-3,
        batch_size: int = 64,
        device: str = 'cpu',
    ):
        self.model = model.to(device)
        self.device = device
        self.batch_size = batch_size

        self.optimizer = optim.Adam(model.parameters(), lr=lr)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, patience=5, factor=0.5
        )
        self.criterion = nn.CrossEntropyLoss()

    def load_data(self, path: str, val_split: float = 0.1):
        """Load training data from HDF5."""
        with h5py.File(path, 'r') as f:
            states = np.array(f['states'])
            actions = np.array(f['actions'])

        # Shuffle
        indices = np.random.permutation(len(states))
        states = states[indices]
        actions = actions[indices]

        # Split
        split_idx = int(len(states) * (1 - val_split))

        self.train_states = torch.FloatTensor(states[:split_idx])
        self.train_actions = torch.LongTensor(actions[:split_idx])
        self.val_states = torch.FloatTensor(states[split_idx:])
        self.val_actions = torch.LongTensor(actions[split_idx:])

        print(f"Train samples: {len(self.train_states)}")
        print(f"Val samples: {len(self.val_states)}")

    def train(
        self,
        epochs: int = 50,
        patience: int = 10,
        checkpoint_dir: str = 'checkpoints',
    ) -> dict:
        """Run training loop."""
        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)

        train_loader = DataLoader(
            TensorDataset(self.train_states, self.train_actions),
            batch_size=self.batch_size,
            shuffle=True,
        )

        best_val_loss = float('inf')
        no_improve = 0
        history = {'train_loss': [], 'val_loss': [], 'val_acc': []}

        for epoch in range(epochs):
            # Training
            self.model.train()
            train_loss = 0.0

            for states, actions in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
                states = states.to(self.device)
                actions = actions.to(self.device)

                self.optimizer.zero_grad()

                # Forward pass (no mask during training - predicting from all actions)
                logits = self.model.policy_head(
                    self.model.backbone(states)
                )

                loss = self.criterion(logits, actions)
                loss.backward()
                self.optimizer.step()

                train_loss += loss.item()

            train_loss /= len(train_loader)

            # Validation
            val_loss, val_acc = self._validate()

            # LR scheduling
            self.scheduler.step(val_loss)

            # Logging
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)

            print(f"Epoch {epoch+1}: train_loss={train_loss:.4f}, "
                  f"val_loss={val_loss:.4f}, val_acc={val_acc:.2%}")

            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                no_improve = 0
                self.model.save(f"{checkpoint_dir}/best_model.pt")
            else:
                no_improve += 1
                if no_improve >= patience:
                    print(f"Early stopping at epoch {epoch+1}")
                    break

        return history

    def _validate(self):
        """Run validation."""
        self.model.eval()
        val_loader = DataLoader(
            TensorDataset(self.val_states, self.val_actions),
            batch_size=self.batch_size,
        )

        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for states, actions in val_loader:
                states = states.to(self.device)
                actions = actions.to(self.device)

                logits = self.model.policy_head(
                    self.model.backbone(states)
                )

                loss = self.criterion(logits, actions)
                total_loss += loss.item()

                preds = logits.argmax(dim=1)
                correct += (preds == actions).sum().item()
                total += len(actions)

        return total_loss / len(val_loader), correct / total


# CLI interface
if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, required=True)
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--batch-size', type=int, default=64)
    parser.add_argument('--lr', type=float, default=1e-3)
    args = parser.parse_args()

    model = PolicyNetwork()
    trainer = ImitationTrainer(model, lr=args.lr, batch_size=args.batch_size)
    trainer.load_data(args.data)
    history = trainer.train(epochs=args.epochs)
```

## Verification

```bash
cd backend

# Generate small dataset first
python -m training.data_generator --games 1000 --output data/train.h5

# Train
python -m training.imitation_trainer --data data/train.h5 --epochs 20

# Verify checkpoint
ls checkpoints/best_model.pt
```

## Reference

- [AI Roadmap - Imitation Learning](../../../ai/ROADMAP.md#21-imitation-learning-clone-the-teacher)

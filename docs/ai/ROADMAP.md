# Acquire AI Roadmap

A linear progression from heuristics to intelligent game-playing agents.

## Vision

Build AI agents that:
1. Provide engaging opponents at multiple skill levels
2. Can participate in trading negotiations naturally
3. Are explainable (players understand why moves were made)

---

## Roadmap Overview

```
Phase 0              Phase 1              Phase 2              Phase 3
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
Rule-Based       â†’   MCTS            â†’   RL               â†’   RL + LLM
(DONE)               (Search)            (DPO/PPO)            Negotiation
```

| Phase | Approach | What It Adds | Status |
|-------|----------|--------------|--------|
| 0 | Rule-based bots | Playable opponents | âœ… Done |
| 1 | MCTS | Stronger play via search | ðŸ“‹ Next |
| 2 | RL (DPO or PPO) | Learning, faster inference | ðŸ“‹ Planned |
| 3 | RL + LLM | Natural trading negotiation | ðŸ“‹ Planned |

---

## Current State

### What We Have

| Component | Status | Location |
|-----------|--------|----------|
| Rule-based bot (easy/medium/hard) | âœ… Complete | `backend/game/bot.py` |
| Unified Action representation | âœ… Complete | `backend/game/action.py` |
| Deterministic game seeding | âœ… Complete | `backend/game/game.py` |
| Game cloning for simulation | âœ… Complete | `backend/game/game.py` |
| Legal action enumeration | âœ… Complete | `backend/game/rules.py` |
| State encoder design | âœ… Documented | `docs/ai/state_encoding.md` |
| Training config | âœ… Complete | `backend/training/config.py` |

---

## Phase 0: Rule-Based âœ… COMPLETE

**Goal**: Playable bots at multiple difficulty levels

**Delivered**:
- Heuristic scoring for all 5 decision types (tile, founding, merger, disposition, buying)
- Three difficulty levels (easy/medium/hard)
- Deterministic mode for reproducibility

**Strength**: Hard bot provides reasonable play using hand-crafted rules

---

## Phase 1: MCTS (Search-Based)

**Goal**: Stronger bot through search, no machine learning required

### Approach

Monte Carlo Tree Search explores possible futures by simulation:

```python
class MCTSBot:
    def __init__(self, simulations: int = 1000):
        self.simulations = simulations

    def choose_action(self, game, player_id) -> Action:
        legal_actions = Rules.get_all_legal_actions(game, player_id)
        wins = {a: 0 for a in legal_actions}
        plays = {a: 0 for a in legal_actions}

        for _ in range(self.simulations):
            action = self.select_action(wins, plays)  # UCB1
            game_copy = game.clone()
            game_copy.apply_action(action)
            winner = self.rollout(game_copy)  # Random playout
            plays[action] += 1
            if winner == player_id:
                wins[action] += 1

        return max(legal_actions, key=lambda a: wins[a] / max(plays[a], 1))
```

### Key Features

| Feature | Description |
|---------|-------------|
| UCB1 selection | Balance exploration vs exploitation |
| Random rollouts | Simulate games to completion |
| Configurable strength | More simulations = stronger play |
| Information Set MCTS | Handle hidden opponent tiles |

### Deliverables

- [ ] `backend/game/mcts_bot.py` - Core MCTS implementation
- [ ] Configurable difficulty via simulation count
- [ ] Benchmark against rule-based bots

### Success Criteria

- MCTS(1000) beats hard bot >60% of games
- Move time <5 seconds with 1000 simulations

---

## Phase 2: Reinforcement Learning (DPO or PPO)

**Goal**: Learn strong play from data, fast inference

### Option A: DPO (Direct Preference Optimization)

Simpler than PPO - no reward model, no value network.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Generate Preference Data                           â”‚
â”‚  - Play MCTS vs weaker bots                                 â”‚
â”‚  - MCTS move = "chosen", weaker move = "rejected"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Train with DPO Loss                                â”‚
â”‚  - Increase probability of chosen moves                     â”‚
â”‚  - Decrease probability of rejected moves                   â”‚
â”‚  - Single loss function, stable training                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**DPO Loss**:
$$\mathcal{L}_{\text{DPO}} = -\log \sigma\left(\beta \log \frac{\pi_\theta(a_w|s)}{\pi_{\text{ref}}(a_w|s)} - \beta \log \frac{\pi_\theta(a_l|s)}{\pi_{\text{ref}}(a_l|s)}\right)$$

### Option B: PPO (Proximal Policy Optimization)

More complex but well-understood. See `docs/ai/ppo_explained.md` for details.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Components                                                  â”‚
â”‚  - Policy network Ï€(a|s)                                    â”‚
â”‚  - Value network V(s)                                        â”‚
â”‚  - GAE for advantage estimation                              â”‚
â”‚  - Clipped objective for stability                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparison

| Aspect | DPO | PPO |
|--------|-----|-----|
| Complexity | Lower | Higher |
| Training stability | Very stable | Requires tuning |
| Data requirement | Preference pairs | Online interaction |
| Reward model | Not needed | Not needed |
| Value network | Not needed | Required |

**Recommendation**: Start with DPO. Simpler to implement, stable training, can leverage MCTS as the "expert" for generating preference data.

### Deliverables

- [ ] `backend/training/state_encoder.py` - State â†’ tensor
- [ ] `backend/training/policy_network.py` - Neural network
- [ ] `backend/training/dpo_trainer.py` - DPO training loop
- [ ] `backend/game/neural_bot.py` - Bot using trained network
- [ ] `backend/training/evaluator.py` - Benchmarking

### Success Criteria

- Beat hard bot >70% of games
- Beat MCTS(1000) >55% of games
- Inference <10ms per move

---

## Phase 3: RL + LLM Negotiation

**Goal**: Add natural language trading to the RL bot

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Hybrid Bot                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  RL Policy (from Phase 2)                             â”‚  â”‚
â”‚  â”‚  - Tile placement                                      â”‚  â”‚
â”‚  â”‚  - Stock purchases                                     â”‚  â”‚
â”‚  â”‚  - Merger decisions                                    â”‚  â”‚
â”‚  â”‚  - Fast inference (<10ms)                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            +                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM Trading Agent                                     â”‚  â”‚
â”‚  â”‚  - Evaluate trade offers                               â”‚  â”‚
â”‚  â”‚  - Propose trades                                      â”‚  â”‚
â”‚  â”‚  - Natural language negotiation                        â”‚  â”‚
â”‚  â”‚  - Explainable decisions                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### LLM Trading Agent

```python
class LLMTradingAgent:
    def evaluate_offer(self, game, offer) -> tuple[bool, str]:
        """Evaluate incoming trade offer."""
        prompt = f"""You are playing Acquire. Evaluate this trade offer.

Game state:
- Your money: ${game.current_player.money}
- Your stocks: {game.current_player.stocks}
- Active chains: {describe_chains(game)}

Trade offer: {describe_offer(offer)}

Consider:
1. Relative stock values based on chain sizes
2. Your majority/minority positions
3. Game stage (early/mid/late)

Respond: ACCEPT or REJECT, then explain briefly."""

        response = llm.complete(prompt)
        return parse_response(response)

    def propose_trade(self, game, player_id) -> Optional[TradeOffer]:
        """Generate a trade proposal if beneficial."""
        prompt = f"""You are playing Acquire. Consider proposing a trade.

Your position: {describe_position(game, player_id)}
Other players: {describe_opponents(game, player_id)}

If a mutually beneficial trade exists, propose it.
If not, respond "NO_TRADE".

Format: TRADE [your stocks] FOR [their stocks] WITH [player]"""

        response = llm.complete(prompt)
        return parse_trade_proposal(response)
```

### Integration

```python
class HybridBot:
    def __init__(self):
        self.rl_policy = load_trained_policy()
        self.llm_trader = LLMTradingAgent()

    def choose_action(self, game, player_id):
        # RL handles core gameplay
        return self.rl_policy.choose_action(game, player_id)

    def handle_trade_offer(self, game, offer):
        # LLM handles trading
        accept, reasoning = self.llm_trader.evaluate_offer(game, offer)
        return accept

    def maybe_propose_trade(self, game, player_id):
        # LLM proposes trades when beneficial
        return self.llm_trader.propose_trade(game, player_id)
```

### Deliverables

- [ ] `backend/llm/trading_agent.py` - LLM trading logic
- [ ] `backend/llm/prompts.py` - Prompt templates
- [ ] `backend/game/hybrid_bot.py` - Combined RL + LLM bot
- [ ] Integration with game WebSocket API

### Success Criteria

- Bot participates in trading (previously declined all trades)
- Trade decisions are reasonable (evaluated by human review)
- Trading adds strategic depth without slowing gameplay

---

## Evaluation Framework

### Benchmark Suite

```python
def run_benchmarks():
    bots = {
        "easy": RuleBot("easy"),
        "medium": RuleBot("medium"),
        "hard": RuleBot("hard"),
        "mcts_100": MCTSBot(100),
        "mcts_1000": MCTSBot(1000),
        "rl_v1": RLBot("v1"),
        "hybrid": HybridBot(),
    }

    # Round-robin tournament
    results = {}
    for bot_a, bot_b in combinations(bots, 2):
        wins_a, wins_b = play_matches(bot_a, bot_b, n=1000)
        results[(bot_a, bot_b)] = (wins_a, wins_b)

    return compute_elo_ratings(results)
```

### Metrics by Phase

| Phase | Key Metric | Target |
|-------|------------|--------|
| 1 (MCTS) | Win rate vs hard bot | >60% |
| 2 (RL) | Win rate vs MCTS(1000) | >55% |
| 2 (RL) | Inference time | <10ms |
| 3 (Hybrid) | Trades accepted/proposed | >0 per game |
| 3 (Hybrid) | Human evaluation of trades | "Reasonable" |

---

## File Structure

```
backend/
â”œâ”€â”€ game/
â”‚   â”œâ”€â”€ bot.py              # âœ… Rule-based bots
â”‚   â”œâ”€â”€ mcts_bot.py         # ðŸ“‹ Phase 1: MCTS
â”‚   â”œâ”€â”€ neural_bot.py       # ðŸ“‹ Phase 2: RL bot
â”‚   â””â”€â”€ hybrid_bot.py       # ðŸ“‹ Phase 3: RL + LLM
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ config.py           # âœ… Hyperparameters
â”‚   â”œâ”€â”€ state_encoder.py    # ðŸ“‹ State â†’ tensor
â”‚   â”œâ”€â”€ policy_network.py   # ðŸ“‹ Neural network
â”‚   â”œâ”€â”€ dpo_trainer.py      # ðŸ“‹ DPO training
â”‚   â”œâ”€â”€ ppo_trainer.py      # ðŸ“‹ PPO training (optional)
â”‚   â””â”€â”€ evaluator.py        # ðŸ“‹ Benchmarking
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ trading_agent.py    # ðŸ“‹ Phase 3: LLM trading
â”‚   â””â”€â”€ prompts.py          # ðŸ“‹ Prompt templates
docs/ai/
â”œâ”€â”€ README.md               # âœ… Overview
â”œâ”€â”€ ROADMAP.md              # âœ… This document
â”œâ”€â”€ ppo_explained.md        # âœ… PPO deep dive
â”œâ”€â”€ state_encoding.md       # âœ… Observation design
â”œâ”€â”€ training_pipeline.md    # âœ… Training details
â””â”€â”€ alternatives/
    â”œâ”€â”€ README.md           # âœ… RL alternatives analysis
    â””â”€â”€ llm-rl-advances.md  # âœ… LLM-RL techniques (DPO, GRPO)
```

Legend: âœ… Complete | ðŸ“‹ Planned

---

## Summary

| Phase | What | Why | Effort |
|-------|------|-----|--------|
| **0** | Rule-based | Baseline opponents | âœ… Done |
| **1** | MCTS | Strong play via search, no ML | 1-2 days |
| **2** | RL (DPO/PPO) | Learn from data, fast inference | 1-2 weeks |
| **3** | RL + LLM | Natural trading negotiation | 1 week |

**Next step**: Implement MCTS (Phase 1) - provides immediate strength improvement with no ML infrastructure required.
